{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aca247",
   "metadata": {},
   "source": [
    "## Script for gen files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fccbc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/vtwv0p951k11cf7tvhd750jw0000gn/T/ipykernel_22854/1613910955.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_inner['quantity_sum'] = result_inner.filter(like='quantity').sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# Directory where raw CSV files are stored\n",
    "directory = \"../jupyter_notebook/data_samples\"\n",
    "\n",
    "# Parsing date strings, ignoring any timezone information and converting them to datetime objects\n",
    "date_parser = lambda x: pd.to_datetime(x[:22])\n",
    "\n",
    "# List to hold all the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Dictionary to hold all the dataframes\n",
    "dict_of_dfs = {}\n",
    "\n",
    "#Types of energy that we considered renewable\n",
    "energy_type_codes_to_filter = ['B01', 'B09', 'B10', 'B11', 'B12', 'B13', 'B15','B16', 'B18', 'B19']\n",
    "\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    # Verifies that files we are reading are the ones of the list\n",
    "    match = re.match(r'gen_[A-Z]{2}_(\\w{3})\\.csv', filename)\n",
    "\n",
    "    if match:\n",
    "        \n",
    "        # Extract the name of the energy_type\n",
    "        energy_type = match.group(1)\n",
    "\n",
    "        #If the energy that we are searching matches with the file it continues\n",
    "        if energy_type in energy_type_codes_to_filter:\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(os.path.join(directory, filename), converters={'EndTime': date_parser})\n",
    "\n",
    "            # Extract country and energy type from filename\n",
    "            _, country, energy_type = filename.split('_')\n",
    "            energy_type = energy_type.replace('.csv', '') # Remove the file extension\n",
    "\n",
    "            # Add country and energy type as new columns\n",
    "            df['CountryCode'] = country\n",
    "            df['EnergyTypeCode'] = energy_type\n",
    "            \n",
    "            #Drop duplicates (duplicates have NaN values in columns)\n",
    "            df.dropna(inplace=True)\n",
    "\n",
    "            #EndTime to datetime datatype\n",
    "            df['EndTime'] = pd.to_datetime(df['EndTime'])\n",
    "\n",
    "            # Difference between periods.\n",
    "            df['TimeDifference'] = df['EndTime'].diff().dt.total_seconds() / 60\n",
    "\n",
    "            # For each datasets consider the smallest value as the period\n",
    "            sampling_period = int(df.loc[(df['TimeDifference'] > 0) & (df['TimeDifference'] <= 60), 'TimeDifference'].min())\n",
    "\n",
    "            # Set \"EndTime\" as index\n",
    "            df.set_index('EndTime', inplace=True)\n",
    "            \n",
    "            #Verifier for 15,30,60 min periods (can be delete)\n",
    "            sampling_period=str(sampling_period)\n",
    "            df = df.resample(f'{sampling_period}T').asfreq()\n",
    "            \n",
    "            # We make groups for each hour, so we can detect where are missing values\n",
    "            df['DateHour'] = df.index.floor('H')\n",
    "            \n",
    "            # Reset_index\n",
    "            df.reset_index('EndTime', inplace=True)\n",
    "\n",
    "            #Group by DateHour and sum to see if the quantity is positive\n",
    "            grouped_df = df.groupby('DateHour')['quantity'].sum().reset_index()\n",
    "            \n",
    "            grouped_df = grouped_df.rename(columns={'quantity': 'HourlySum'})\n",
    "\n",
    "            df = pd.merge(df, grouped_df, how='left', left_on='DateHour', right_on='DateHour')\n",
    "            \n",
    "            # We stay with values that have a group (HoyrlySum!=0) and that are NaN values (quantity NaN)\n",
    "            df=df[~((df['quantity'].isnull())&(df['HourlySum']==0))]\n",
    "\n",
    "            df.interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "\n",
    "            df.set_index('EndTime',inplace=True)\n",
    "\n",
    "            numeric_cols = df.select_dtypes(include=['number'])\n",
    "            categorical_cols = df.select_dtypes(exclude=['number', 'datetime64[ns]', 'bool'])\n",
    "\n",
    "            # Resample the numeric columns and sum\n",
    "            resampled_df_num = numeric_cols.resample('H').sum()\n",
    "\n",
    "            # Resample the categorical columns.\n",
    "            # Here, we take the first value. Adjust the method if needed (e.g., 'last', or a custom function to get the mode)\n",
    "            resampled_df_cat= categorical_cols.resample('H').last()\n",
    "\n",
    "            resampled_df = pd.concat([resampled_df_num, resampled_df_cat], axis=1)\n",
    "\n",
    "            resampled_df.dropna(inplace=True)\n",
    "\n",
    "            # Append the dataframe to the list\n",
    "            #dataframes.append(resampled_df)\n",
    "\n",
    "            dict_of_dfs[f'{country}_{energy_type}']=resampled_df\n",
    "\n",
    "            # Concatenate all dataframes (if needed)\n",
    "            #final_df = pd.concat (dataframes)\n",
    "\n",
    "# Concatenate all dataframes (if needed)\n",
    "#final_df = pd.concat(dataframes)\n",
    "\n",
    "# From the EDA (no info given)\n",
    "dataframes_to_drop = ['SP_B10', 'SE_B13']\n",
    "\n",
    "for dataframe_name in dataframes_to_drop:\n",
    "    dict_of_dfs.pop(dataframe_name, None)\n",
    "\n",
    "#We group by countries.\n",
    "dic_gen = {}\n",
    "for name, df in dict_of_dfs.items():\n",
    "    gen_key = name.split('_')[0]\n",
    "    if gen_key not in dic_gen:\n",
    "        dic_gen[gen_key] = {}  # Inicializa un diccionario vacÃ­o para esta clave si no existe\n",
    "    df.rename(columns={'quantity':f'quantity_{name}'},inplace=True)\n",
    "    dic_gen[gen_key][name] = df[[f'quantity_{name}']]\n",
    "\n",
    "dict_of_dfs_gen={}\n",
    "\n",
    "for name,df in dic_gen.items():\n",
    "    dataframes_inner = list(dic_gen[name].values())\n",
    "    result_inner = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='inner'), dataframes_inner)\n",
    "    \n",
    "    result_inner['quantity_sum'] = result_inner.filter(like='quantity').sum(axis=1)\n",
    "    \n",
    "    dict_of_dfs_gen[name]=result_inner[['quantity_sum']]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a9f1e",
   "metadata": {},
   "source": [
    "## Script for load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4387c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory where raw CSV files are stored\n",
    "directory = \"../jupyter_notebook/data_samples\"\n",
    "# Parsing date strings, ignoring any timezone information and converting them to datetime objects\n",
    "date_parser = lambda x: pd.to_datetime(x[:22])\n",
    "# List to hold all the dataframes\n",
    "dict_of_dfs_load = {}\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if re.match(r'load_[A-Z]{2}+\\.csv', filename):\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(directory, filename), converters={'EndTime': date_parser}).set_index('EndTime')\n",
    "        \n",
    "        _, country = filename.split('_')\n",
    "        \n",
    "        country = country.replace('.csv', '') # Remove the file extension\n",
    "                \n",
    "        numeric_cols = df.select_dtypes(include=['number'])\n",
    "        categorical_cols = df.select_dtypes(exclude=['number', 'datetime64[ns]', 'bool'])\n",
    "        \n",
    "        # Resample the numeric columns and sum\n",
    "        resampled_df_num = numeric_cols.resample('H').sum()\n",
    "\n",
    "        # Resample the categorical columns.\n",
    "        # Here, we take the first value. Adjust the method if needed (e.g., 'last', or a custom function to get the mode)\n",
    "        resampled_df_cat= categorical_cols.resample('H').last()\n",
    "        \n",
    "        resampled_df = pd.concat([resampled_df_num, resampled_df_cat], axis=1)\n",
    "        \n",
    "        dict_of_dfs_load[country]=resampled_df[['Load']]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a4299",
   "metadata": {},
   "source": [
    "## Surplus calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9a4533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation=dict_of_dfs_gen.copy()\n",
    "load=dict_of_dfs_load.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "088e251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict={}\n",
    "for name in generation:\n",
    "    result=pd.merge(generation[name],load[name],left_index=True,right_index=True,how='inner')\n",
    "    final_dict[name]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0987b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "surplus_per_country={}\n",
    "for name in final_dict:\n",
    "    final_dict[name]['surplus']=final_dict[name]['quantity_sum']-final_dict[name]['Load']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bb504",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5fffccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(final_dict.values(), keys=final_dict.keys(), names=['country_code'])\n",
    "\n",
    "# Reiniciar el Ã­ndice si es necesario\n",
    "concatenated_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c76cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.to_csv('surplus_base.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
